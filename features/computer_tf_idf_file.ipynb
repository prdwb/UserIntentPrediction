{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import *\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "def compute_idf(sents_ints, term_to_id_dict, id_to_term_dict):\n",
    "    term_to_idf_dict = dict()\n",
    "    term_to_df_dict = dict()\n",
    "    term_to_tf_dict = dict() # return term_to_tf_dict to compute the background probability in QL\n",
    "    total_doc_number = 0.0\n",
    "    stop = set(stopwords.words('english')) # remove stop words\n",
    "\n",
    "    for sent_ints in sents_ints:\n",
    "        sent = [id_to_term_dict[i] for i in sent_ints if id_to_term_dict[i] not in stop]\n",
    "        update_df_dict(sent, term_to_df_dict, id_to_term_dict, term_to_tf_dict)\n",
    "        total_doc_number += 1.0\n",
    "\n",
    "    for term in term_to_df_dict:\n",
    "        idf = math.log((total_doc_number - term_to_df_dict[term] + 0.5) / (term_to_df_dict[term] + 0.5))\n",
    "        term_to_idf_dict[term] = idf\n",
    "\n",
    "    # normlize term_to_tf_dict\n",
    "    total_token_num = float(sum(term_to_tf_dict.values()))\n",
    "    for t in term_to_tf_dict.keys():\n",
    "        term_to_tf_dict[t] /= total_token_num\n",
    "\n",
    "    return term_to_idf_dict, term_to_tf_dict\n",
    "\n",
    "def update_df_dict(q1, term_to_df_dict, id_to_term_dict, term_to_tf_dict):\n",
    "    word_set = set(q1)\n",
    "    for w in q1:\n",
    "        if w in term_to_tf_dict:\n",
    "            term_to_tf_dict[w] += 1.0\n",
    "        else:\n",
    "            term_to_tf_dict[w] = 1.0\n",
    "        word_set.add(w)\n",
    "    for w in word_set:\n",
    "        if w in term_to_df_dict:\n",
    "            term_to_df_dict[w] += 1.0\n",
    "        else:\n",
    "            term_to_df_dict[w] = 1.0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    vocab_file = 'data/vocab.tsv'\n",
    "    idf_file = 'data/idf.tsv'\n",
    "\n",
    "    term_to_id_dict = dict()\n",
    "    id_to_term_dict = dict()\n",
    "    init_term_to_id_dict(term_to_id_dict, vocab_file)\n",
    "    id_to_term_dict = dict(zip(term_to_id_dict.values(), term_to_id_dict.keys()))\n",
    "\n",
    "    conn_title = connect_db()\n",
    "    conn_utter = connect_db()\n",
    "\n",
    "    sql_title = 'select title from titles_final'\n",
    "    sql_utter = 'select utterance from contents_final'\n",
    "\n",
    "    with conn_title.cursor() as cursor_title, conn_utter.cursor() as cursor_utter:\n",
    "        cursor_title.execute(sql_title)\n",
    "        titles = [row['title'] for row in cursor_title.fetchall()]\n",
    "\n",
    "        cursor_utter.execute(sql_utter)\n",
    "        utterances = [row['utterance'] for row in cursor_utter.fetchall()]\n",
    "\n",
    "    sents_ints = []\n",
    "    sents = map(clean_str, titles + utterances)\n",
    "    for sent in sents:\n",
    "        sent_ints = [term_to_id_dict.get(term) for term in tokenizer(sent)]\n",
    "        sent_ints = list(filter(lambda x: x is not None, sent_ints))\n",
    "        sents_ints.append(sent_ints)\n",
    "\n",
    "    term_to_idf_dict, term_to_tf_dict = compute_idf(sents_ints, term_to_id_dict, id_to_term_dict)\n",
    "\n",
    "    with open(idf_file, 'w') as f_out:\n",
    "        for term in term_to_idf_dict:\n",
    "            f_out.write(term + '\\t' + str(term_to_idf_dict[term]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
